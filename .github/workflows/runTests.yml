# This is a test pipeline for the API tests
name: API Benchmark Tests
# Controls when the workflow will run
on:
  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:
    inputs:
      environment:
        required: true
        description: Select the environment to execute on
        type: choice 
        options: 
          - ops
          - uat
      runtime:
        required: true
        description: Define how long the benchmarking should last. Examples 5s, 2m, 1h 3m 
        default: 10s
        type: string 
      threadcount:
        required: true
        description: Select how many times the tests should run at the same time
        default: 1
        type: number
      

jobs:
  # First job in the workflow installs and verifies the software
  build:
    name: Test Execution
    # The type of runner that the job will run on
    runs-on: ubuntu-latest
    steps:
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it and the directory won't be empty
      - uses: actions/checkout@v2
      - uses: actions/setup-python@v2
        with:
          python-version: 3.9
      - name: Install dependencies
        run: |
          pip3 install --upgrade pip
          pip3 install -r ./dependencies_pip
      - name: Execute tests
        run: |
          sh run_local.sh "${{ github.event.inputs.environment }}" "${{ github.event.inputs.runtime }}" "${{ github.event.inputs.threadcount }}"
      - name: Publish Test Results
        uses: EnricoMi/publish-unit-test-result-action@v2
        if: always()
        with:
          files: |
            test-results/**/*.index
      - name: Upload Artifact
        uses: actions/upload-pages-artifact@v1
        # uses: actions/upload-artifact@v4
        with: 
          path: './test-results/locust_tests_report.html'
          name: 'Locust-Report'
      - name: Deploy Artifact
        uses: actions/deploy-pages@v2
        id: deployment 
      
